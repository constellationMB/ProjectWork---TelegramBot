import requests
import spacy
from openai import OpenAI
from loguru import logger

from configuration import settings

LANGUAGETOOL_API_URL = "https://api.languagetool.org/v2/check"


def rename(text: str) -> str:
    tag_mapping = {
        'PRON': '–ú–µ—Å—Ç–æ–∏–º–µ–Ω–∏–µ',
        'NOUN': '–°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ',
        'PROPN': '–°–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ',
        'ADJ': '–ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ',
        'ADV': '–ù–∞—Ä–µ—á–∏–µ',
        'VERB': '–ì–ª–∞–≥–æ–ª',
        'AUX': '–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –≥–ª–∞–≥–æ–ª',
        'ADP': '–ü—Ä–µ–¥–ª–æ–≥',
        'NUM': '–ß–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–µ',
        'DET': '–ê—Ä—Ç–∏–∫–ª—å',
        'PART': '–ß–∞—Å—Ç–∏—Ü–∞',
        'SYM': '–°–∏–º–≤–æ–ª',
        'INTJ': '–ú–µ–∂–¥–æ–º–µ—Ç–∏–µ',
        'PUNCT': '–ó–Ω–∞–∫ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è',
        'X': '–ü—Ä–æ—á–µ–µ',
        'CCONJ': '–°–æ—é–∑',
        'SCONJ': '–°—É–±—Å—Ç–∞–Ω—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–æ—é–∑',
    }

    return tag_mapping.get(text, text)


def check_text_with_languagetool(text: str) -> list:
    try:
        payload = {
            'text': text,
            'language': 'ru'
        }
        response = requests.post(LANGUAGETOOL_API_URL, data=payload)
        result = response.json()

        if 'matches' not in result or len(result['matches']) == 0:
            return ["<b>–û—à–∏–±–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç ‚úÖüéâ</b>", False]

        corrections = []
        for i, match in enumerate(result['matches'], start=1):
            message = (
                f"<b>‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî</b>"
                f"‚ú® <b>–û—à–∏–±–∫–∞ #{i}</b>\n"
                f"üîç <b>–¢–∏–ø:</b> {match['shortMessage'] or '–ù–µ —É–∫–∞–∑–∞–Ω–æ'}\n"
                f"üìã <b>–û–ø–∏—Å–∞–Ω–∏–µ:</b> {match['message']}\n"
                f"üìñ <b>–ü—Ä–∞–≤–∏–ª–æ:</b> {match['rule']['description']}\n"
                f"‚úèÔ∏è <b>–ö–æ–Ω—Ç–µ–∫—Å—Ç:</b> {match['context']['text']}\n"
                f"‚û°Ô∏è <b>–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ:</b> "
                f"{', '.join([m['value'] for m in match['replacements'][:3]]) if match['replacements'] else '–ù–µ –Ω–∞–π–¥–µ–Ω–æ / –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}\n"
            )
            corrections.append(message)

        return ["\n\n".join(corrections), True]

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é LanguageTool: {e}")
        return ["‚ö†Ô∏è –ê –µ–≥–æ –Ω–µ—Ç, LanguageTool –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ( ò Õü ñ  ò)", False]


def check_and_improve_with_languagetool(text: str) -> str:
    url = "https://api.languagetool.org/v2/check"
    payload = {
        "text": text,
        "language": "ru"
    }

    try:
        response = requests.post(url, data=payload)
        response.raise_for_status()
        matches = response.json().get("matches", [])

        if not matches:
            return "üéâ –¢–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—à–∏–±–æ–∫ –∏–ª–∏ —É–ª—É—á—à–µ–Ω–∏–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è."

        corrected_text = text
        explanations = []

        for match in matches:
            start = match["offset"]
            end = start + match["length"]
            replacement = match["replacements"][0]["value"] if match["replacements"] else None
            if replacement:
                corrected_text = corrected_text[:start] + replacement + corrected_text[end:]
                explanations.append(f"üîÑ {text[start:end]} ‚Üí {replacement}: {match['message']}")

        return f"‚ú® –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {corrected_text}\n\nüìã –ü–æ—è—Å–Ω–µ–Ω–∏—è:\n" + "\n".join(explanations)

    except:
        logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é API LanguageTool")
        return "‚ö†Ô∏è –ê –µ—ë –Ω–µ—Ç, LanguageTool —Å–ª–æ–º–∞–ª—Å—è ÔºûÔ∏øÔºú"


def check_text_with_spacy(text: str) -> str:
    try:
        nlp = spacy.load("ru_core_news_sm")
        doc = nlp(text)
        sentences = list(doc.sents)

        results = []
        for sentence in sentences:
            sentence_result = []
            for token in sentence:
                if token.is_punct or token.is_space:
                    continue
                sentence_result.append(f"üîπ {token.text} - {rename(token.pos_)}")
            results.append("\n".join(sentence_result))

        return "üìù –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:\n\n" + "\n\n".join(results)

    except:
        logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é SpaCy")
        return "‚ö†Ô∏è –ê –µ–≥–æ –Ω–µ—Ç, SpaCy –ø—Ä–∏–∫–∞–∑–∞–ª –∂–∏—Ç—å –¥–æ–ª–≥–æ <( _ _ )>"


def check_text_with_openai(text: str, model: str = "gpt-3.5-turbo", max_tokens: int = 200) -> str:
    try:
        client = OpenAI(api_key=settings.CHAT_GPT_TOKEN)

        response = client.completions.create(
            model=model,
            prompt=[
                "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –≤–∏–¥–µ —Ç–µ–∫—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –µ–≥–æ –Ω–∞ –æ—à–∏–±–∫–∏ –∏ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –∏—Ö, –µ—Å–ª–∏ –æ–Ω–∏ –∏–º–µ—é—Ç—Å—è. "
                "–¢–∞–∫ –∂–µ, –µ—Å–ª–∏ —ç—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, —Ç–æ –ø—Ä–µ–¥–ª–æ–∂–∏ —Å–≤–æ—é –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∫—É, –µ—Å–ª–∏ –æ–Ω–∞ –Ω—É–∂–Ω–∞. "
                "–ï—Å–ª–∏ –µ—Å—Ç—å –æ—à–∏–±–∫–∏ –≤ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏, –∏—Å–ø—Ä–∞–≤–ª—è–π. –ö–∞–∂–¥—É—é –æ—à–∏–±–∫—É –∫—Ä–∞—Ç–∫–æ –ø–æ—è—Å–Ω—è–π.",
                text
            ],
            max_tokens=max_tokens,
            temperature=0.5
        )

        return f"‚ú® –û—Ç–≤–µ—Ç ChatGPT:\n\n{response.choices[0].message.content}"

    except:
        logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é ChatGPT")
        return "‚ö†Ô∏è –ê –µ–≥–æ –Ω–µ—Ç, ChatGPT –≤ –†–æ—Å—Å–∏–∏ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω ‡≤•_‡≤•"
